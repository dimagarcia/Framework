@book{dey2010bayesian,
  title="{Bayesian Modeling in Bioinformatics}",
  author="{Dey, D.K. and Ghosh, S. and Mallick, B.K.}",
  isbn="{9781420070187}",
  lccn="{2009049470}",
  series="{Chapman \& Hall/CRC Biostatistics Series}",
  url="{https://books.google.com.co/books?id=AOOvLpr6mn4C}",
  year="{2010}",
  publisher="{CRC Press}"
}
@book{korb2010bayesian,
  title={Bayesian Artificial Intelligence, Second Edition},
  author={Korb, K.B. and Nicholson, A.E.},
  isbn={9781439815922},
  series={Chapman \& Hall/CRC Computer Science \& Data Analysis},
  url={https://books.google.com.co/books?id=LxXOBQAAQBAJ},
  year={2010},
  publisher={CRC Press}
}
@book{KollerFriedman09,
  title = {Probabilistic Graphical Models: Principles and Techniques},
  author = {D. Koller and N. Friedman},
  year = 2009,
  publisher = {MIT Press},
}
@article{madigan1994model,
  title={Model selection and accounting for model uncertainty in graphical models using Occam's window},
  author={Madigan, David and Raftery, Adrian E},
  journal={Journal of the American Statistical Association},
  volume={89},
  number={428},
  pages={1535--1546},
  year={1994},
  publisher={Taylor \& Francis Group}
}
@article{madigan1995bayesian,
  title={Bayesian graphical models for discrete data},
  author={Madigan, David and York, Jeremy and Allard, Denis},
  journal={International Statistical Review/Revue Internationale de Statistique},
  pages={215--232},
  year={1995},
  publisher={JSTOR}
}
@article{Giudici01121999,
author = {Giudici, P and Green, PJ}, 
title = {Decomposable graphical Gaussian model determination},
volume = {86}, 
number = {4}, 
pages = {785-801}, 
year = {1999}, 
doi = {10.1093/biomet/86.4.785}, 
abstract ={We propose a methodology for Bayesian model determination in decomposable graphical Gaussian models. To achieve this aim we consider a hyper inverse Wishart prior distribution on the concentration matrix for each given graph. To ensure compatibility across models, such prior distributions are obtained by marginalisation from the prior conditional on the complete graph. We explore alternative structures for the hyperparameters of the latter, and their consequences for the model. Model determination is carried out by implementing a reversible jump Markov chain Monte Carlo sampler. In particular, the dimension-changing move we propose involves adding or dropping an edge from the graph. We characterise the set of moves which preserve the decomposability of the graph, giving a fast algorithm for maintaining the junction tree representation of the graph at each sweep. As state variable, we use the incomplete variance-covariance matrix, containing only the elements for which the corresponding element of the inverse is nonzero. This allows all computations to be performed locally, at the clique level, which is a clear advantage for the analysis of large and complex datasets. Finally, the statistical and computational performance of the procedure is illustrated by mean of both artificial and real datasets.}, 
URL = {http://biomet.oxfordjournals.org/content/86/4/785.abstract}, 
eprint = {http://biomet.oxfordjournals.org/content/86/4/785.full.pdf+html}, 
journal = {Biometrika} 
}
@article{Metropolis1953,
   author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
   title = {Equation of State Calculations by Fast Computing Machines},
   journal = {The Journal of Chemical Physics},
   year = {1953},
   volume = {21},
   number = {6}, 
   pages = "1087-1092",
   url = "http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114",
   doi = "http://dx.doi.org/10.1063/1.1699114" 
}
@article{HASTINGS01041970,
author = {HASTINGS, W. K.}, 
title = {Monte Carlo sampling methods using Markov chains and their applications},
volume = {57}, 
number = {1}, 
pages = {97-109}, 
year = {1970}, 
doi = {10.1093/biomet/57.1.97}, 
abstract ={SUMMARY A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.}, 
URL = {http://biomet.oxfordjournals.org/content/57/1/97.abstract}, 
eprint = {http://biomet.oxfordjournals.org/content/57/1/97.full.pdf+html}, 
journal = {Biometrika} 
}
@article{Friedman2003Bayesian,
year={2003},
issn={0885-6125},
journal={Machine Learning},
volume={50},
number={1-2},
doi={10.1023/A:1020249912095},
title={Being Bayesian About Network Structure. A Bayesian Approach to Structure Discovery in Bayesian Networks},
url={http://dx.doi.org/10.1023/A%3A1020249912095},
publisher={Kluwer Academic Publishers},
keywords={Bayesian networks; structure learning; MCMC; Bayesian model averaging},
author={Friedman, Nir and Koller, Daphne},
pages={95-125},
language={English}
}
@Inbook{Chickering1996,
author={Chickering, David Maxwell},
title="Learning Bayesian Networks is NP-Complete",
bookTitle="Learning from Data: Artificial Intelligence and Statistics V",
year="1996",
publisher="Springer New York",
address="New York, NY",
pages="121--130",
isbn="978-1-4612-2404-4",
doi="10.1007/978-1-4612-2404-4_12",
url="http://dx.doi.org/10.1007/978-1-4612-2404-4_12"
}
@article{EllisAndWong2008,
author = {Byron Ellis and Wing Hung Wong},
title = {Learning Causal Bayesian Network Structures From Experimental Data},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {482},
pages = {778-789},
year = {2008},
doi = {10.1198/016214508000000193},

URL = { 
        http://dx.doi.org/10.1198/016214508000000193
    
},
eprint = { 
        http://dx.doi.org/10.1198/016214508000000193
    
}
,
abstract = { We propose a method for the computational inference of directed acyclic graphical structures given data from experimental interventions. Order-space Markov chain Monte Carlo, equi-energy sampling, importance weighting, and stream-based computation are combined to create a fast algorithm for learning causal Bayesian network structures. }
}