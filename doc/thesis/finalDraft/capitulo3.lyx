#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass classicthesis
\options spanish
\use_default_options true
\master Thesis.lyx
\maintain_unincluded_children false
\language spanish
\language_package babel
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\is_math_indent 0
\math_numbering_side default
\quotes_style french
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes true
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\author 2089656932 "User"
\end_header

\begin_body

\begin_layout Chapter

\lang american
Aprendizaje de una red Bayesiana óptima y sus parámetros a partir de datos
 de expresión génica
\end_layout

\begin_layout Section

\lang american
Introducción
\end_layout

\begin_layout Standard

\lang american
La identificación y el análisis de genes de interes biológico y sus interaciones
 son claves en diseño de farmacos, mejoramiento fisiologico de plantas e
 investigación de enfermedades como la oncogenesis, entre otros.
 Es posible analizar el comportamiento colectivo de grupos de genes usando
 modelos que representen la interacción entre ellos.
 
\change_inserted 2089656932 1563628494
A
\change_unchanged
lgunos modelos son las redes de co-expresión génica y las redes regulatorias
 de genes, donde se observan patrones de expresión que siguen los grupos
 de genes bajo ciertas condiciones biológicas dadas
\change_inserted 2089656932 1563628546
 
\change_unchanged
y el impacto del comportamiento de un grupo sobre otros.
 Estas redes han sido representadas con modelos de redes Bayesianase e inferida
 su estructura y parámetros con métodos estadísticos como las simulaciones
 de Markov Chain MonteCarlo (MCMC) e inferencia Bayesiana.
\end_layout

\begin_layout Standard

\lang american
El proposito de este cap
\change_inserted 2089656932 1563628568
í
\change_deleted 2089656932 1563628568
i
\change_unchanged
tulo es caracterizar un método para el aprendizaje de una red Bayesiana
 y sus parámetros a partir de da
\change_deleted 2089656932 1564916132
q
\change_unchanged
tos de expresión y mostrar su funcionamiento con algunos ejemplos como el
 problema de cancer de pulm
\change_inserted 2089656932 1563628677
ó
\change_deleted 2089656932 1563628677
o
\change_unchanged
n y datos de expresión g
\change_inserted 2089656932 1563628692
é
\change_deleted 2089656932 1563628692
e
\change_unchanged
nica de E.coli.
 Primero caracterizamos un método general de aprendizaje de redes Bayesianas
 adaptando el algor
\change_deleted 2089656932 1563628713
t
\change_unchanged
i
\change_inserted 2089656932 1563628718
t
\change_unchanged
mo de 
\change_inserted 2089656932 1563628728
M
\change_deleted 2089656932 1563628727
m
\change_unchanged
etropolis Hasting, luego presentamos una versión iterativa de esta adaptación
 para obtener conjuntos de redes 
\change_inserted 2089656932 1563628782
ó
\change_deleted 2089656932 1563628782
o
\change_unchanged
ptimas m
\change_inserted 2089656932 1563628787
á
\change_deleted 2089656932 1563628786
a
\change_unchanged
s amplios.
 Posteriormente, trataremos el tema del sobre-entrenamiento de las redes
 obtenidas y presentamos diferentes estrategías para mitigar esta situación.
 Estas estra
\change_inserted 2089656932 1563628850
te
\change_unchanged
gias comprenden desde restricciones como establecer un máximo in-grade durante
 la simulaci
\change_inserted 2089656932 1563628909
ó
\change_deleted 2089656932 1563628909
o
\change_unchanged
n MCMC ó un valor minimo de dependencia entre nodos padres e hijos (near-indepen
dence) hasta estrategías pos-
\change_inserted 2089656932 1563628943
ó
\change_deleted 2089656932 1563628943
o
\change_unchanged
ptimas como la obtención de una red Bayesiana minimal y una reducida que
 faciliten los análisis y permitan dar con redes 
\change_inserted 2089656932 1563629000
ó
\change_deleted 2089656932 1563628998
o
\change_unchanged
ptimas coherentes desde el punta de vista biológic
\change_inserted 2089656932 1563629022
o
\change_deleted 2089656932 1563629021
a
\change_unchanged
 para el caso del ejemplo con E.coli.
 Finalmente proporcionamos unos comentarios conluyentes
\change_inserted 2089656932 1563629119
.

\change_unchanged
 
\change_deleted 2089656932 1563629116
donde se evidencia la predictibilidad de las estructuras de redes Bayesianas
 minimales dependiendo del tamaño de la red según el número de nodos.
 Tamnbién, se resalta la importancia de contar con información biológica
 como las anotaciones GO para poder validar la coherencia de los resultados.3
\change_unchanged

\end_layout

\begin_layout Section
Caracterización de un método general de aprendizaje de redes Bayesianas
 y sus parámetros.
\end_layout

\begin_layout Standard
En est
\change_inserted 2089656932 1563643061
a
\change_deleted 2089656932 1563643060
e
\change_unchanged
 
\change_inserted 2089656932 1563643071
sección 
\change_deleted 2089656932 1563643065
capítulo
\change_unchanged
 se presenta la caracterización de un método de aprendizaje de redes Bayesianas
 y sus parámetros basados en el enfoque de simulaciones de Markov Chain
 MonteCarlo (MCMC, para mayor detalle ver 
\begin_inset CommandInset citation
LatexCommand cite
key "Hastings1970"
literal "false"

\end_inset

 y 
\begin_inset CommandInset citation
LatexCommand cite
key "ISI:000178037200004"
literal "false"

\end_inset

).
 Este método será parte del marco de trabajo de redes Bayesianas que se
 propondrá y se compone de los siguientes pasos: 
\end_layout

\begin_layout Enumerate
Definir la red Bayesiana inicial e inicializar la matriz de adyacencia 
\begin_inset Formula $NxN$
\end_inset

 correspondiente, siendo 
\begin_inset Formula $N$
\end_inset

 el número de variables o nodos de la red Bayesiana.
 Por defecto, la red se inicializa como una red totalmente desconectada
 y se convierte en 
\emph on
la red Bayesiana actual
\emph default
.
\end_layout

\begin_layout Enumerate
Evaluar el ajuste de los datos dada la red Bayesiana 
\emph on
actual
\emph default
, es decir, cálcular la función de 
\emph on
verosimilitud
\emph default
 para el modelo de la red Bayesiana 
\emph on
actual
\emph default
.
\end_layout

\begin_layout Enumerate
Buscar una estructura de red Bayesiana 
\emph on
candidata
\emph default
 en el vecindario de la red Bayesiana 
\emph on
actual
\emph default
.
\end_layout

\begin_layout Enumerate
Evaluar la función 
\emph on
de verosimilitud
\emph default
 para la red Bayesiana 
\emph on
candidata
\emph default
.
\end_layout

\begin_layout Enumerate
Calcular la probabilidad de aceptación de la red Bayesiana candidata dada
 la red Bayesiana actual.
\end_layout

\begin_layout Enumerate
Aplicar el 
\emph on
criterio de aceptación
\emph default
 de la red Bayesiana 
\emph on
candidata
\emph default
 basado en el algortimo de 
\emph on
Metropolis Hasting
\emph default
.
\end_layout

\begin_layout Enumerate
Iterar los pasos del 3.
 al 6.
 hasta alcanzar el 
\emph on
número máximo de simulaciones
\emph default
, parametrizado previamente.
\end_layout

\begin_layout Standard
Para efectos de la caracterización de este método, se establec
\change_inserted 2089656932 1563654416
ieron
\change_deleted 2089656932 1563654388
era
\change_unchanged
 la
\change_inserted 2089656932 1563654056
s
\change_unchanged
 siguiente
\change_inserted 2089656932 1563654058
s
\change_unchanged
 
\change_deleted 2089656932 1563654060
notación
\change_unchanged
 
\change_inserted 2089656932 1564917021
consideraciones 
\change_unchanged
para la representación de
\change_inserted 2089656932 1563654069
l
\change_unchanged
 
\change_inserted 2089656932 1563654075
modelo de 
\change_unchanged
una red Bayesiana
\change_inserted 2089656932 1563654182
 y un conjunto de muestras subyacentes de este modelo.
 Sea 
\begin_inset Formula $B$
\end_inset

 un modelo de red Bayesiana con
\change_unchanged
 
\begin_inset Formula $B=\left(G,\Theta\right)$
\end_inset


\change_inserted 2089656932 1563653769
, donde 
\begin_inset Formula $G$
\end_inset

 es un grafo acíclico dirigido (GAD)
\change_unchanged
 cuyos nodos
\change_inserted 2089656932 1563653444
 
\change_deleted 2089656932 1563653444
 en 
\begin_inset Formula $G$
\end_inset

 
\change_unchanged
corresponden a un conjunto de variables aleatorias
\change_inserted 2089656932 1563654209
 discretas
\change_unchanged
 
\begin_inset Formula $V=\left\{ V_{1},\ldots,V_{N}\right\} $
\end_inset

 
\change_deleted 2089656932 1563654205
discretas
\change_inserted 2089656932 1563654536
, 
\begin_inset Formula $\Theta$
\end_inset

 un vector, donde cada elemento representa una probabilidad condicional
\change_unchanged
 
\change_inserted 2089656932 1563654498
entre las variables aleatorias de 
\begin_inset Formula $G$
\end_inset


\change_deleted 2089656932 1563654252
y
\change_inserted 2089656932 1563654550
 y 
\begin_inset Formula $X$
\end_inset


\change_unchanged
 un conjunto dado de 
\change_deleted 2089656932 1563654562

\begin_inset Formula $M$
\end_inset


\change_inserted 2089656932 1563654744

\begin_inset Formula $M$
\end_inset


\change_unchanged
 observaciones idéntica e independientemente distribuidas 
\emph on
(IID)
\emph default
 
\begin_inset Formula $X=\left\{ X_{1},\ldots,X_{M}\right\} $
\end_inset

 
\change_inserted 2089656932 1563654645
y 
\change_unchanged
obtenidas a partir de un experimento estadístico cuyo modelo 
\change_deleted 2089656932 1563654820
estadístico
\change_unchanged
 subyacente, 
\change_deleted 2089656932 1564916969
se
\change_unchanged
 
\change_deleted 2089656932 1563654918
cree la hipótesis
\change_unchanged
 puede ser explicado por 
\begin_inset Formula $B$
\end_inset


\change_inserted 2089656932 1563654927
.
 Adicionalmente
\change_unchanged
:
\end_layout

\begin_layout Itemize
Sea 
\begin_inset Formula $A=\left[a_{ij}\right]$
\end_inset

 la matriz de adyacencia para representar la estructura de una red Bayesiana,
 donde 
\begin_inset Formula $a_{ij}$
\end_inset

toma el valor 
\begin_inset Formula $1$
\end_inset

 si existe una arista entre los nodos 
\begin_inset Formula $i$
\end_inset

 y 
\begin_inset Formula $j$
\end_inset

 de la red ó 
\begin_inset Formula $0$
\end_inset

 en caso contrario.
\end_layout

\begin_layout Itemize
Sea 
\begin_inset Formula $P\left(X|G,\Theta\right)$
\end_inset

 la función de verosimilitud correspondiente al modelo de la red Bayesiana
 
\begin_inset Formula $B$
\end_inset

.
\end_layout

\begin_layout Itemize
Sea 
\begin_inset Formula $P\left(G,\Theta|X\right)$
\end_inset

 la distribución de la probabilidad del modelo de la red Bayesiana 
\begin_inset Formula $B$
\end_inset

, posterior a las observaciones 
\begin_inset Formula $X$
\end_inset

.
\end_layout

\begin_layout Standard
A continuación, se profundiza sobre algunos conceptos y operaciones involucrados
 en los diferentes pasos del método de aprendizaje de redes Bayesianas propuesto
 en esta investigación.
\end_layout

\begin_layout Subsection
La red Bayesiana inicial e inicialización de la matriz de adyacencia
\end_layout

\begin_layout Standard
Ya que la estructura de la red Bayesiana inicial 
\begin_inset Formula $G$
\end_inset

 será desconexa su respectiva matriz de adyacencia 
\begin_inset Formula $A$
\end_inset

 de 
\begin_inset Formula $NxN$
\end_inset

 deberá inicializarse así: 
\begin_inset Formula $a_{ij}=0$
\end_inset

, para 
\begin_inset Formula $i=1\ldots N$
\end_inset

 y 
\begin_inset Formula $j=1\ldots N$
\end_inset

.
 Es posible iniciar la simulación con una red conectada, sin embargo, la
 estructura inicial dada podría sesgar la caminata aleatoria a traves del
 espacio de búsqueda de estructuras.
\end_layout

\begin_layout Subsection
La función de 
\emph on
verosimilitud
\emph default
 para el modelo de la red Bayesiana
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1563655002
Generalmente, 
\change_deleted 2089656932 1563655005
S
\change_inserted 2089656932 1563655006
s
\change_unchanged
e 
\emph on
optimiz
\change_inserted 2089656932 1563655011
a
\change_deleted 2089656932 1563655010
ó
\change_unchanged
 el cálculo computacional
\emph default
 de 
\change_deleted 2089656932 1563656010
estas
\change_inserted 2089656932 1563656010
las
\change_unchanged
 funciones
\change_inserted 2089656932 1563656021
 de verosimilitud
\change_unchanged
 modificando la 
\change_deleted 2089656932 1563655063
concavidad
\change_inserted 2089656932 1563656036
su escala
\change_unchanged
 
\change_inserted 2089656932 1563656134
de su valores de evaluación
\change_deleted 2089656932 1563656031
de las mismas
\change_unchanged
 
\change_deleted 2089656932 1563656114
por medio
\change_inserted 2089656932 1563656115
usando
\change_unchanged
 
\change_deleted 2089656932 1563656065
de
\change_unchanged
 la función logaritmo natural
\change_inserted 2089656932 1564919202
, en nuestro caso
\change_unchanged
: 
\begin_inset Formula $ln\left(P\left(X|G,\Theta\right)\right)$
\end_inset

.
 Por ejemplo, 
\change_inserted 2089656932 1563656230
supongamos que queremos estimar un parámetro 
\begin_inset Formula $Theta$
\end_inset

 usando 
\change_deleted 2089656932 1564919235
para la función de
\change_unchanged
 una distribución
\series bold
 
\change_inserted 2089656932 1563655082

\series default
\emph on
D
\change_deleted 2089656932 1563655082
d
\change_unchanged
irichlet
\emph default
 con parámetros 
\begin_inset Formula $\alpha_{1}=20$
\end_inset

 y 
\begin_inset Formula $\alpha_{2}=29$
\end_inset

, como se puede observar en la Figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Concavidad"
plural "false"
caps "false"
noprefix "false"

\end_inset


\change_inserted 2089656932 1563656241
,
\change_unchanged
 
\change_inserted 2089656932 1563656257
si 
\change_deleted 2089656932 1563656261
su transformación
\change_inserted 2089656932 1564919300
 transformamos esta distribución
\change_unchanged
 
\change_inserted 2089656932 1563656285
empleando 
\change_unchanged
e
\change_inserted 2089656932 1563656287
l
\change_deleted 2089656932 1563656287
n
\change_unchanged
 logaritmo natural
\change_inserted 2089656932 1564919310
, dicha transformación
\change_unchanged
 arroja
\change_inserted 2089656932 1563656304
rá
\change_unchanged
 valores más pequeños que la función
\change_inserted 2089656932 1564919321
 original
\change_unchanged
 evaluada 
\change_deleted 2089656932 1564919336
incluso
\change_unchanged
 en el punto pico 
\begin_inset Formula $0.4$
\end_inset

 del parámetro 
\change_deleted 2089656932 1563656732

\emph on
Theta
\change_inserted 2089656932 1564919356

\emph default

\begin_inset Formula $\theta$
\end_inset

 estimado
\change_unchanged
.

\change_inserted 2089656932 1563657599
 Esta técnica facilita computacionalmente los calculos ya que cuando se
 tienen productos de variables, como pasa en las distribuciones conjuntas
 de los modelos de redes Bayesianas, se mitigan factores de riesgo como
 el desbordamiento de la capacidad de variables de cálculo por excesivas
 operaciones de multiplicación, y esta situación es todavia más crítica
 si el modelo incluye muchas variables de estudio, además, al calcular productos
 de variables dentro de un logaritmo estos producto se transforman en sumas
 alivianando los calculos de maquina.
\change_unchanged

\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_deleted 2089656932 1563656743
PDF
\change_inserted 2089656932 1563656743

\begin_inset Formula $PDF$
\end_inset


\change_unchanged
 y 
\change_deleted 2089656932 1563656748
Ln(PDF)
\change_inserted 2089656932 1563656748

\begin_inset Formula $Ln(PDF)$
\end_inset


\change_unchanged
 vs Parámetro 
\change_deleted 2089656932 1563656751
Theta
\change_inserted 2089656932 1563656768

\begin_inset Formula $\theta$
\end_inset

.
\change_unchanged

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/Fig-Concavidad.png
	scale 50

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset

 
\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Concavidad"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
El vecindario de la red Bayesiana 
\emph on
actual
\end_layout

\begin_layout Standard
La conectividad entre dos estructuras de red vecinas en el espacio de búsqueda,
 está dada en términos de los siguientes operadores: 
\emph on
(i)
\emph default
 
\emph on
agregar una arista
\emph default
 a la estructura de la red actual, 
\emph on
(ii)
\emph default
 
\emph on
quitar una arista
\emph default
 a la estructura de red actual y 
\emph on
(iii)
\emph default
 
\emph on
quitar una arista
\emph default
 a la estructura de la red actual y a la red resultante 
\emph on
agregar una arista
\emph default
 (
\emph on
doble operación
\emph default
).
 Este enfoque está basado en el trabajo de 
\begin_inset CommandInset citation
LatexCommand cite
key "ISI:000178037200004"
literal "false"

\end_inset


\change_inserted 2089656932 1563662216
, Sin embargo se modificó este enfoque para poder determinar de antemano
 el número de vecinos de una estructura y seleccionar aleatoriamente una
 de ellas.
 Esto evita explorar algunas estructuras candidatas que al analizarlas presentan
 ciclos.
 Por eso modificamos la operación 
\emph on
(iii)
\emph default
 que originalmente consiste en invertir una arista.
\change_unchanged
.
 
\change_deleted 2089656932 1563662243

\emph on
Para hacer este
\change_inserted 2089656932 1563662243
El
\change_unchanged
 procedimiento estocástico 
\change_inserted 2089656932 1563662254
que aplicamos 
\change_deleted 2089656932 1563662281
se
\change_unchanged
 hace conteo del número de estructuras para cada operador (i), (ii) y (iii)
 y aleatóriamente 
\change_deleted 2089656932 1563662311
se
\change_unchanged
 selecciona una
\emph default
.
 La red Bayesiana 
\emph on
candidata
\emph default
 será obtenida aplicando uno de los tres operadores anteriores y se denotará
 como 
\begin_inset Formula $B^{'}=\left(G^{'},\Theta^{'}\right)$
\end_inset

 y su matriz de adyacencia 
\begin_inset Formula $A^{'}.$
\end_inset

 A contiuación, se detallara cada una de las operaciones.\SpecialChar allowbreak

\end_layout

\begin_layout Subsubsection
(i) O
\series bold
btención de las posibles estructuras de redes acíclicas después de agregar
 una arista
\series default
.
 
\end_layout

\begin_layout Standard
La matriz de adjacencia 
\begin_inset Formula $A$
\end_inset

 indica los caminos de longitud 
\begin_inset Formula $1$
\end_inset

 entre cada par de nodos 
\begin_inset Formula $i,j$
\end_inset

 donde 
\begin_inset Formula $a_{ij}=1$
\end_inset

, y se puede obtener los caminos de longitud 
\begin_inset Formula $2,\ldots,N-1$
\end_inset

 calculando las potencias de 
\begin_inset Formula $A$
\end_inset

, es decir, 
\begin_inset Formula $A^{2},\ldots,A^{N-1}$
\end_inset

.
 Además, si se toman las traspuestas de dichas matrices y se suman con la
 matriz identidad 
\begin_inset Formula $I$
\end_inset

 de dimensión 
\begin_inset Formula $NxN$
\end_inset

, se tiene la siguiente matriz resultante: 
\begin_inset Formula 
\[
C=[c_{ij}],c_{ij}=\begin{cases}
1 & Existe\begin{array}{ccc}
camino & entre & j\end{array},i\\
0 & \begin{array}{ccc}
No & existe & camino\end{array}
\end{cases}
\]

\end_inset

Así, al agregar una arista entre los nodos 
\begin_inset Formula $i$
\end_inset

 y 
\begin_inset Formula $j$
\end_inset

 donde 
\begin_inset Formula $c_{ij}=1$
\end_inset

, se crea un bucle entre ellos.
 Por consiguiente, para obtener una estructura aciclica, 
\series bold
solo se debe tener en cuenta los nodos 
\begin_inset Formula $i$
\end_inset

 y 
\begin_inset Formula $j$
\end_inset

 donde 
\begin_inset Formula $c_{ij}=0$
\end_inset

 al momento de agregar nuevas aristas, si es que no se quiere generar ciclos
\series default
.
 E
\change_deleted 2089656932 1563662405
n ese orden de ideas, e
\change_unchanged
l cálculo de 
\begin_inset Formula $C$
\end_inset

 está dado de la siguiente manera: 
\begin_inset Formula 
\[
C=I+Transpuesta(A)+Transpuesta(A^{2})+\ldots+Transpuesta(A^{N-1})
\]

\end_inset


\change_inserted 2089656932 1563662432
,y el número de posibilidades de agregar una arista (manteniendo la red
 acíclica) es igual al número de 0's en 
\begin_inset Formula $C$
\end_inset

.
 
\change_unchanged

\end_layout

\begin_layout Subsubsection

\series bold
(ii)
\series default
 Eliminación de una arista.
 
\end_layout

\begin_layout Standard
Se debe tomar un par de nodos 
\begin_inset Formula $i,j$
\end_inset

 tales que 
\begin_inset Formula $a_{ij}=1$
\end_inset

 e invertir su valor 
\begin_inset Formula $a_{ij}=0$
\end_inset


\change_inserted 2089656932 1563662577
.
 El número de posibilidades de eliminar una arista entonces es igual al
 número de 1's en 
\begin_inset Formula $A$
\end_inset

.
\change_unchanged

\end_layout

\begin_layout Subsubsection

\series bold
(iii) 
\series default
La 
\series bold
doble operación
\end_layout

\begin_layout Standard
Consiste en aplicar el operador (ii) 
\change_deleted 2089656932 1563662614
e inmediatamente
\change_inserted 2089656932 1563662619
 y luego
\change_unchanged
 
\change_inserted 2089656932 1563662629
aplicar el operador (i) 
\change_unchanged
a la estructura resultante 
\change_deleted 2089656932 1563662627
aplicar el operador (i)
\change_unchanged
.

\change_inserted 2089656932 1563662666
 El número de posibles estructuras vecinas está dado por las suma de 0's
 en las matrices 
\begin_inset Formula $C^{*}$
\end_inset

 que resultan de cada posible eliminación de una arista de la estructura
 original.
\change_unchanged

\end_layout

\begin_layout Subsection
Evaluación de la función 
\emph on
de verosimilitud
\emph default
 para la red Bayesiana 
\emph on
candidata
\end_layout

\begin_layout Standard
Teniendo en cuenta que la estructura de la red Bayesiana 
\emph on
candidata
\emph default
 cambi
\change_inserted 2089656932 1564919690
a
\change_deleted 2089656932 1564919690
ó
\change_unchanged
 ligeramente con respecto a la red Bayesiana 
\emph on
actual
\change_inserted 2089656932 1564919711

\emph default
 desde donde es generada
\change_unchanged
, la evaluación de la función de 
\emph on
verosimilitud
\emph default
 para la red Bayesiana 
\emph on
candidata
\emph default
 
\begin_inset Formula $P\left(X|G^{'},\Theta^{'}\right)$
\end_inset

 se optimizó empleando una técnica de 
\emph on
cache
\emph default
 guardando los valores de la función calculados para la red Bayesiana 
\emph on
actual
\emph default
 y teniendo en cuenta la transformación de la función logaritmo natural
 
\begin_inset Formula $ln$
\end_inset

 
\change_inserted 2089656932 1564919750
, que se mencionó anteriormente
\change_unchanged
.
 En ese orden ideas se cálcula un delta 
\begin_inset Formula $\Delta$
\end_inset

 entre los valores de evaluación de la función de la red Bayesiana 
\emph on
actual
\emph default
 
\emph on
versus
\emph default
 la 
\emph on
candidata
\emph default
, así, 
\begin_inset Formula $\Delta=ln\left(P\left(X|G^{'},\Theta^{'}\right)\right)-ln\left(P\left(X|G,\Theta\right)\right)$
\end_inset

 
\change_inserted 2089656932 1564919788
, en este caso 
\change_unchanged
para la función de verosimilitud.
\end_layout

\begin_layout Subsection
La probabilidad de aceptación de la red Bayesiana candidata dada la red
 Bayesiana actual
\end_layout

\begin_layout Standard
Tomando como base el algortimo de
\emph on
 Metropolis Hasting (MH) 
\begin_inset CommandInset citation
LatexCommand cite
key "Hastings1970"
literal "false"

\end_inset

, se calcula el factor 
\change_deleted 2089656932 1563662853
alpha
\change_unchanged
 
\begin_inset Formula $\alpha$
\end_inset


\emph default
 para efectos del criterio de aceptación que nos permite evaluar la red
 Bayesiana 
\emph on
candidata
\emph default
 de la siguiente manera
\change_inserted 2089656932 1563662862
,
\change_unchanged
 empleando la evaluación de la función de verosimilitud: 
\begin_inset Formula 
\[
\alpha=\frac{P\left(X|G^{'},\Theta^{'}\right)P\left(G,\Theta|G^{'},\Theta^{'}\right)}{P\left(X|G,\Theta\right)P\left(G^{'},\Theta^{'}|G,\Theta\right)}
\]

\end_inset


\begin_inset Formula $P\left(G^{'},\Theta^{'}|G,\Theta\right)$
\end_inset

 denota la probabilidad de la transición de la red Bayesiana 
\emph on
actual
\emph default
 a la red Bayesiana 
\emph on
candidata
\emph default
 
\begin_inset Formula $G,\Theta\rightarrow G^{'},\Theta^{'}$
\end_inset

 y se cálcula así: 
\begin_inset Formula 
\[
P\left(G^{'},\Theta^{'}|G,\Theta\right)=\frac{1}{númeroDeEstructurasVecinas\left(G\right)}
\]

\end_inset

, donde 
\begin_inset Formula $númeroDeEstructurasVecinas\left(G\right)$
\end_inset

 
\change_deleted 2089656932 1563663136
como se definió en 3.
 
\change_unchanged
es 
\emph on
el conteo del número de estructuras posibles al aplicar los operadores (i),
 (ii) y (iii)
\change_deleted 2089656932 1564919872
 
\change_inserted 2089656932 1564919870
, 
\change_unchanged
y ya que aleatóriamente se selecciona un
\change_inserted 2089656932 1564919919
a
\change_deleted 2089656932 1564919917
o
\change_inserted 2089656932 1564919924
 estructura
\change_unchanged
, 
\emph default
tal probabilidad será 1 dividido por dicho conteo.
 Siguiendo un razonamiento similar se llega al siguiente cálculo: 
\begin_inset Formula 
\[
\beta=\frac{P\left(G,\Theta|G^{'},\Theta^{'}\right)}{\boldsymbol{P\left(G^{'},\Theta^{'}|G,\Theta\right)}}=\frac{número-de-estructuras\left(G,\Theta\rightarrow G^{'},\Theta^{'}\right)}{número-de-estructuras\left(G^{'},\Theta^{'}\rightarrow G,\Theta\right)}
\]

\end_inset

, y así se tiene que criterio 
\begin_inset Formula $\alpha$
\end_inset

 será entoces: 
\begin_inset Formula 
\[
\alpha=\frac{P\left(X|G^{'},\Theta^{'}\right)}{P\left(X|G,\Theta\right)}*\beta
\]

\end_inset

Finalmente, teniendo en cuenta la transformación de la función a logaritmo
 natural el cálculo de la probabilidad de aceptación de la red Bayesiana
 
\emph on
candidata
\change_inserted 2089656932 1564919954
,
\change_unchanged

\emph default
 se
\change_deleted 2089656932 1564919971
rá
\change_unchanged
 
\change_inserted 2089656932 1564919991
calculó de esta manera
\change_deleted 2089656932 1564919993
así
\change_unchanged
: 
\begin_inset Formula 
\[
ln\left(\alpha\right)=ln\left(P\left(X|G^{'},\Theta^{'}\right)\right)-ln\left(P\left(X|G,\Theta\right)\right)+ln\left(\beta\right)
\]

\end_inset


\end_layout

\begin_layout Subsection
Aplicación del 
\emph on
criterio de aceptación
\emph default
 de la red Bayesiana 
\emph on
candidata
\emph default
 basado en el algortimo de 
\emph on
Metropolis Hasting
\end_layout

\begin_layout Standard

\emph on
El paso final del algoritmo MH es evaluar el criterio de aceptación
\emph default
.
 La evaluación en 
\change_deleted 2089656932 1563663501
MH
\change_inserted 2089656932 1563663518
 el algoritmo de 
\emph on
Metropolis Hasting
\change_unchanged

\emph default
 
\change_deleted 2089656932 1563663524
siendo
\change_unchanged
 
\change_inserted 2089656932 1563663529
se hace para 
\change_unchanged

\begin_inset Formula $r=min(1,\alpha)$
\end_inset

 y
\change_inserted 2089656932 1563663544
 un aleatorio
\change_unchanged
 
\begin_inset Formula $u\sim U\left(0,1\right)$
\end_inset


\change_inserted 2089656932 1563663584
:
\change_unchanged
 se acepta la transición 
\begin_inset Formula $G,\varTheta\rightarrow G^{'},\Theta^{'},$
\end_inset

 
\begin_inset Formula $si\left(u<r\right)$
\end_inset

, es decir: 
\begin_inset Formula 
\[
G\rightarrow G^{'},\varTheta\rightarrow\varTheta^{'};si\left(u<r\right)
\]

\end_inset


\change_deleted 2089656932 1563663824
Ya que anteriormente obtuvimos 
\begin_inset Formula $ln\left(\alpha\right)$
\end_inset

 y al ser 
\begin_inset Formula $\alpha$
\end_inset

 la probabilidad de aceptación se debe cumplir que 
\begin_inset Formula $\left(0\leq\alpha\leq1\right)$
\end_inset

 y 
\begin_inset Formula $\left(ln\left(\alpha\right)\leq0\right)$
\end_inset

, puesto que 
\begin_inset Formula $ln\left(1\right)=0$
\end_inset

, y entonces, se debe modificar también la obtención de 
\begin_inset Formula $r$
\end_inset

, así
\change_inserted 2089656932 1563663826
 Para ajustar lo anterior al cálculo logarítmico se hace
\change_unchanged
: 
\begin_inset Formula 
\[
ln\left(r\right)=min\left(0,ln\left(\alpha\right)\right)
\]

\end_inset

.
\begin_inset Newline newline
\end_inset


\change_deleted 2089656932 1563663868
Igualmente, se debe modificar la condición de aceptación de
\change_unchanged
 
\change_inserted 2089656932 1563663864
Y se acepta la transición a 
\change_unchanged
la red Bayesiana 
\emph on
candidata
\emph default
, 
\change_deleted 2089656932 1563663898
por
\change_inserted 2089656932 1563663900
 si 
\begin_inset Formula $ln(u)<ln\text{\_}r$
\end_inset

 para un aleatorio 
\begin_inset Formula $u$
\end_inset


\change_unchanged
: 
\begin_inset Formula 
\[
G\rightarrow G^{'},\varTheta\rightarrow\varTheta^{'};si\left(ln\left(u\right)<ln\left(r\right)\right)
\]

\end_inset


\end_layout

\begin_layout Section

\change_inserted 2089656932 1565003930
Caso discreto
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565003885

\lang american
Ya que los datos de expresión génica no son discretos, se hace necesario
 un procedimiento adicional de discretización para tratar dichos datos.
 Sea 
\lang spanish

\begin_inset Formula $v_{i}$
\end_inset

 una variable aleatoria tal que:
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565003885
\begin_inset Formula 
\[
v_{i}=\begin{cases}
\begin{array}{c}
0\\
1\\
2
\end{array} & \begin{array}{cc}
,if & v_{i}<Q_{i,\frac{1}{3}}\\
,if & Q_{i,\frac{1}{3}}\leq v_{i}<Q_{i,\frac{2}{3}}\\
,if & v_{i}\geq Q_{i,\frac{2}{3}}
\end{array}\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565004174
donde 
\begin_inset Formula $Q_{i,\frac{1}{3}}$
\end_inset

 y 
\begin_inset Formula $Q_{i,\frac{2}{3}}$
\end_inset

 son 
\begin_inset Formula $\frac{1}{3}$
\end_inset

 y 
\begin_inset Formula $\frac{2}{3}$
\end_inset

 cuantiles de los valores de expresión del 
\begin_inset Formula $i-esimo$
\end_inset

 gen; para 
\begin_inset Formula $i=1,...,d$
\end_inset

, así cada variable aleatoria 
\begin_inset Formula $v_{i}$
\end_inset

 tendra una tabla de probabilidad condicional (TPC) con 
\begin_inset Formula $q_{i}=\prod_{v_{p}\in Pa(v_{i})}$
\end_inset

filas y cada entrada corresponde a una combinación diferente de valores
 de valores de las variables padre de 
\begin_inset Formula $v_{i}$
\end_inset

 y adicionalmente, cada fila contiene un vector de probabilidades 
\begin_inset Formula $\theta_{i,j,k}$
\end_inset

 con el valor de probabilidad que 
\begin_inset Formula $v_{i}=j$
\end_inset

 dado que las variables 
\begin_inset Formula $Pa(v_{i})$
\end_inset

 tomen los valores de la 
\begin_inset Formula $k-esima$
\end_inset

 entrada de la TPC correspondiente.
 Por lo tanto, una red Bayesiana para una estructura G será paremetrizada
 así: 
\begin_inset Formula $\Theta=\{\theta_{ijk}>0:\sum_{j}\theta_{ijk}=1\}$
\end_inset

.
\end_layout

\begin_layout Subsection

\change_inserted 2089656932 1565004206
Verosimilitud
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565003885
Dadas 
\begin_inset Formula $N$
\end_inset

 observaciones independientes 
\begin_inset Formula $X=\{X_{1},...,X_{N}\}$
\end_inset

 obtenidas de la distribución conjunta (ver la ecuación 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:jointDistrib"
plural "false"
caps "false"
noprefix "false"

\end_inset

 ), estadístico suficiente para 
\begin_inset Formula $\Theta$
\end_inset

 es el conjunto de conteos 
\begin_inset Formula $\{N_{ijk}\}$
\end_inset

 donde 
\begin_inset Formula $N_{ijk}$
\end_inset

 es el número de veces que 
\begin_inset Formula $v_{i}=j$
\end_inset

 dado que las variables 
\begin_inset Formula $Pa(v_{i})$
\end_inset

 toman los valores especificados en la 
\begin_inset Formula $k-esíma$
\end_inset

 entrada de la TPC de 
\begin_inset Formula $v_{i}$
\end_inset

 (para mayor detalle ver el trabajo de Ellis y Wong en 
\begin_inset CommandInset citation
LatexCommand cite
key "EllisAndWong2008"
literal "false"

\end_inset

).
 En ese orden de ideas los conteos 
\begin_inset Formula $\{N_{ijk},j=0,1,2\}$
\end_inset

 siguen una distribución 
\emph on
multinomial 
\emph default
con parámetros 
\begin_inset Formula $N_{i.k}=N_{i0k}+N_{i1k}+N_{i2k}$
\end_inset

 (intentos) y 
\begin_inset Formula $\{\theta_{i0k},\theta_{i1k},\theta_{i2k}\}$
\end_inset

 (vector de probabilidad).
 Por lo tanto, la función de verosimilitud del modelo de la red Bayesiana
 para el caso discreto es un producto de multinomiales:
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565003885
\begin_inset Formula 
\begin{equation}
P\left(X|G,\Theta\right)=\prod_{i=1}^{d}\prod_{k=1}^{q_{i}}Multinomial(N_{i.k},\theta_{i0k},\theta_{i1k},\theta_{i2k})\label{eq:2-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565003885
donde 
\begin_inset Formula $Multinomial(N_{i.k},\theta_{i0k},\theta_{i1k},\theta_{i2k})=\binom{N_{i.k}}{N_{i0k},N_{i1k},N_{i2k}}\theta_{i0k}^{N_{i0k}}.\theta_{i1k}^{N_{i1k}}.\theta_{i2k}^{N_{i2k}}$
\end_inset

.
\end_layout

\begin_layout Subsection

\change_inserted 2089656932 1565004237
Posterior
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565004236
En la teoría de probabilidad Bayesiana, si las distribuciones 
\emph on
posteriores
\emph default
 
\begin_inset Formula $P(\Theta|X)$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 estan en la misma familia que la distribución de probabilidad 
\family default
\series default
\shape default
\size default
\emph on
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
prior
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset Formula $P(\Theta)$
\end_inset

, la prior y la posterior son llamadas distribuciones conjugadas y la prior
 es llamada una prior conjugada para la función de verosimilitud (para mayor
 detalle ver los trabajos de 
\begin_inset CommandInset citation
LatexCommand cite
key "raiffa1961applied,bernardo2006bayesian,Gelman2003"
literal "false"

\end_inset

), volviendo al caso para una verosimilitud multinomial con parametros 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $N_{i.k}$
\end_inset

 (intentos) and 
\begin_inset Formula $\{\theta_{i0k},\theta_{i1k},\theta_{i2k}\}$
\end_inset

 (vector de probabilidad) según la teoría Bayesiana , la distribución prior
 conjugada es una 
\begin_inset Formula $Dirichlet$
\end_inset

 con hiperparametros 
\begin_inset Formula $\alpha_{i.k}=\{\alpha_{i0k},\alpha_{i1k},\alpha_{i2k}\}$
\end_inset

 y la posterior 
\begin_inset Formula $\{N_{i0k}+\alpha_{i0k},N_{i1k}+\alpha_{i1k},N_{i2k}+\alpha_{i2k}\}$
\end_inset

.
 Por consiguiente la poserior para el modelo de red Bayesiana es un producto
 de 
\begin_inset Formula $Dirichlets$
\end_inset

 así:
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565003885
\begin_inset Formula 
\begin{equation}
P(G,\Theta|X)=\prod_{i=1}^{d}\prod_{k=1}^{q_{i}}Diritchlet(N_{i0k}+\alpha_{i0k},N_{i1k}+\alpha_{i1k},N_{i2k}+\alpha_{i2k})\label{eq:3-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565003885
donde
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565003885
\begin_inset Formula 
\[
Diritchlet(N_{i0k}+\alpha_{i0k},N_{i1k}+\alpha_{i1k},N_{i2k}+\alpha_{i2k})=
\]

\end_inset


\begin_inset Formula 
\[
\frac{\theta_{i0k}^{N_{i0k}+\alpha_{i0k}-1}.\theta_{i1k}^{N_{i1k}+\alpha_{i1k}-1}.\theta_{i2k}^{N_{i2k}+\alpha_{i2k}-1}}{\beta(N_{i0k}+\alpha_{i0k},N_{i1k}+\alpha_{i1k},N_{i2k}+\alpha_{i2k})},
\]

\end_inset

 
\begin_inset Formula 
\[
\theta_{i0k}\approx E[\theta_{i0k}]=\frac{N_{i0k}+\alpha_{i0k}}{(N_{i0k}+\alpha_{i0k}+N_{i1k}+\alpha_{i1k}+N_{i2k}+\alpha_{i2k})},
\]

\end_inset

 
\begin_inset Formula 
\[
\theta_{i1k}\approx E[\theta_{i1k}]=\frac{N_{i1k}+\alpha_{i1k}}{(N_{i0k}+\alpha_{i0k}+N_{i1k}+\alpha_{i1k}+N_{i2k}+\alpha_{i2k})},
\]

\end_inset

 
\begin_inset Formula 
\[
\theta_{i2k}\approx E[\theta_{i2k}]=\frac{N_{i2k}+\alpha_{i2k}}{(N_{i0k}+\alpha_{i0k}+N_{i1k}+\alpha_{i1k}+N_{i2k}+\alpha_{i2k})}
\]

\end_inset

 y 
\begin_inset Formula 
\[
\beta(N_{i0k}+\alpha_{i0k},N_{i1k}+\alpha_{i1k},N_{i2k}+\alpha_{i2k})=
\]

\end_inset


\begin_inset Formula 
\[
\frac{\Gamma(N_{i0k}+\alpha_{i0k})\Gamma(N_{i1k}+\alpha_{i1k})\Gamma(N_{i2k}+\alpha_{i2k})}{\Gamma(N_{i0k}+\alpha_{i0k}+N_{i1k}+\alpha_{i1k}+N_{i2k}+\alpha_{i2k})}.
\]

\end_inset


\end_layout

\begin_layout Subsection

\change_inserted 2089656932 1565008971
Ejemplo y sobreentrenamiento
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565007159
Para este ejemplo se emplearon datos de expresión génica de E.coli tomados
 de 
\begin_inset Formula $M^{3D}$
\end_inset

 (del inglés Many Microbe Microarrays Database), donde se seleccionaron
 aleatoriamente  5 clusters para control y 1 cluster para análisis (transporte
 de lactosa del inglés lactose transport)  para un total de 
\begin_inset Formula $16$
\end_inset

 genes del genoma de E.coli, que se listan a continación con sus respectivas
 anotaciones de ontología del gen (las anotaciones GO fueron tomadas de
 Ibarra en 
\begin_inset CommandInset citation
LatexCommand cite
key "doi:10.1093/database/baw089"
literal "false"

\end_inset

):
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565005008
Lactose transport: 
\begin_inset Newline newline
\end_inset

 lacA – galactoside O-acetyltransferase monomer
\begin_inset Newline newline
\end_inset

 lacI – LacI DNA-binding transcriptional repressor
\begin_inset Newline newline
\end_inset

 lacY – lactose / melibiose:H+ symporter LacY
\begin_inset Newline newline
\end_inset

 lacZ – 
\begin_inset Formula $\beta$
\end_inset

-galactosidase monomer 
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565005008
Global regulators:
\begin_inset Newline newline
\end_inset

 rpoD – RNA polymerase, sigma 70 (sigma D) factor
\begin_inset Newline newline
\end_inset

 hns – H-NS DNA-binding transcriptional dual regulator
\begin_inset Newline newline
\end_inset

 crp – CRP transcriptional dual regulator
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565005008
Zinc ion transport: 
\begin_inset Newline newline
\end_inset

 znuA – Zn2+ ABC transporter - periplasmic binding protein
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565005008
Sodium ion transport: 
\begin_inset Newline newline
\end_inset

 ttdA – L-tartrate dehydratase, 
\begin_inset Formula $\alpha$
\end_inset

 subunit
\begin_inset Newline newline
\end_inset

 ttdB – L-tartrate dehydratase, 
\begin_inset Formula $\beta$
\end_inset

 subunit
\begin_inset Newline newline
\end_inset

 ttdR – Dan
\begin_inset Newline newline
\end_inset

 ttdT – tartrate:succinate antiporter
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565005008
Phosphorelay signal transduction system: 
\begin_inset Newline newline
\end_inset

 zraP – zinc homeostasis protein
\begin_inset Newline newline
\end_inset

 zraR – ZraR transcriptional activator
\begin_inset Newline newline
\end_inset

 zraS – ZraS sensory histidine kinase
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565006925
Response to arsenic-containing substance: 
\begin_inset Newline newline
\end_inset

 arsB – ArsB 
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565008952
El resultado de aplicar el proceso de aprendizaje de redes Bayesianas al
 conjunto de genes seleccionados lo podemos ver en la fígura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:wn_overfit"
plural "false"
caps "false"
noprefix "false"

\end_inset

 donde la tendencia del aprendizaje de redes Bayesianas al 
\emph on
sobre-entrrenamiento
\emph default
 es evidente, ya que independiente de las funciones génicas anotadas en
 la ontología del gen (ver 
\begin_inset CommandInset citation
LatexCommand cite
key "RN58"
literal "false"

\end_inset

) la red resultante relaciona  todos los genes, por su naturaleza a optimizar
 el puntaje de evaluación (score, en esta caso el logaritmo de la posterior).
 En la siguiente sección veremos como podemos darle una vuelta a esta situación
 mitigando esta tendenciatema que se abordará en la siguiente sección.
\change_unchanged

\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565005449
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\change_inserted 2089656932 1565005449
\begin_inset Graphics
	filename images/wn-overfit_Ecoli.png
	scale 250

\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\change_inserted 2089656932 1565005449


\backslash
centering
\end_layout

\end_inset

 
\begin_inset Caption Standard

\begin_layout Plain Layout

\change_inserted 2089656932 1565005449
Tendencia a sobre-entrenamiento del aprendizaje cuando se incluyen genes
 de diferentes clusters de E.coli en una misma simulación MCMC.
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:wn_overfit"

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Section

\change_inserted 2089656932 1565009166
Sobreentrenamiento y la r
\change_deleted 2089656932 1565009164
R
\change_unchanged
ed Bayesiana minimal 
\change_inserted 2089656932 1565012979

\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\change_inserted 2089656932 1563737351
Pasar sobre entrenamiento aqui, y mencionar el intento de las estrategias
 y por que no se adoptaron - criterios para parametrizar estas estrategias
 (max.
 ingrade, near-independence) -> se deja expuesto para aplicar por el experto
 ya que no cabe en un framework general.
\change_unchanged

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565013055
Como vimos en la fígura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:wn_overfit"
plural "false"
caps "false"
noprefix "false"

\end_inset

 de la sección anterior, la tendencia del aprendizaje de redes Bayesianas
 al 
\emph on
sobre-entrrenamiento
\emph default
 es evidente, ya que independiente de las funciones génicas anotadas en
 la ontología del gen (ver 
\begin_inset CommandInset citation
LatexCommand cite
key "RN58"
literal "false"

\end_inset

) la red resultante relaciona  todos los genes, por su naturaleza a optimizar
 el puntaje de evaluación (del inglés score, en esta caso el la función
 logaritmo natural de la posterior).
 
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565013359
Para este caso, después de las simulaciones MCMC, se intentaron diferentes
 transformaciones a las redes óptimas obtenidas para mitigar el sobre-entrenamie
nto, entre las cuales se exploró restringir la propiedad 
\emph on
in-grade
\emph default
 (
\emph on
máx.
 in-grade
\emph default
) a las redes obtenidas y restringir el nivel de dependencia entre los nodos
 hijo y sus padres (
\emph on
near-independence
\emph default
) e incluso combianaciones de estas dos restricciones.
 Sin embargo, los resultados obtenidos no fueron satisfactorios, por esta
 razón se propusó una nueva estrategía pos-optima consistente en construir
 una 
\emph on
red Bayesiana minimal
\emph default
 y un procedimiento de 
\emph on
reducción
\emph default
 que permitío mitigar el sobre-entrenamiento, incluso en experimentos con
 un alto grado de incertidumbre, y aun asi obtener resultados coherentes
 desde el punto de vista biológico como lo presentaremos posteriormente
 en el caso de E.coli (ver sección 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:ResultadosEcoli"
plural "false"
caps "false"
noprefix "false"

\end_inset

)
\end_layout

\begin_layout Subsection

\change_inserted 2089656932 1565012222
Sobreentrenamiento
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565013465
En network science existen métricas y propiedades que soportan el análisis
 de redes complejas (ver la sección 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Background-networksScience"
plural "false"
caps "false"
noprefix "false"

\end_inset

),la propiedad 
\emph on
in-grade
\emph default
 en redes es el número de aristas que un nodo recibe de otros nodos y es
 utilizada como estrategía para evitar el sobre entrenamiento.
 Se definió un parámetro para el valor máximo de 
\emph on
in-grade
\emph default
 para las redes  evaluadas en la simulación MCMC.
 El escenario de aplicación de esta restricción durante la simulación MCMC
 es el siguiente:
\end_layout

\begin_layout Enumerate

\change_inserted 2089656932 1565013541
Antes de adicionar cada arista se evalua si el valor 
\emph on
in-grade
\emph default
 del nodo destino de la arista es menor que el tope máximo parametrizado.
\end_layout

\begin_layout Enumerate

\change_inserted 2089656932 1565013567
En los casos que se cumpla la condición anterior, la simulación continua.
\end_layout

\begin_layout Enumerate

\change_inserted 2089656932 1565013571
En los demás casos, la operación se ignora.
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565010122
Otra alternativa para evitar el sobre-entrenamiento esta basada en el concepto
 de independencia estadística y es aplicable entre nodos hijos y padres
 de cada red obtenida durante la simulacion MCMC.
 Es decir, si el nodo hijo 
\begin_inset Formula $\nu_{j}$
\end_inset

 es impactado durante una simulación para que adicional a sus padres 
\begin_inset Formula $Pa_{v_{j}}$
\end_inset

 se una  
\begin_inset Formula $\nu_{i}$
\end_inset

, y el nuevo conjunto de padres 
\begin_inset Formula $Pa_{v_{j}}\cup\{v_{i}\}$
\end_inset

 son independientes entonces se debe satisfacer la siguiente ecuación:
\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565010122

\emph on
\begin_inset Formula 
\[
P(\nu_{j},Pa_{\nu_{j}}|\nu_{i})-P(\nu_{j}|Pa_{\nu_{j}})=0
\]

\end_inset


\end_layout

\begin_layout Standard

\change_inserted 2089656932 1565013608
Así, si el valor de 
\begin_inset Formula $P(\nu_{j},Pa_{\nu_{j}}|\nu_{i})-P(\nu_{j}|Pa_{\nu_{j}})$
\end_inset

 es cercano a 0, la nueva arista entre 
\begin_inset Formula $v_{i}\rightarrow y_{j}$
\end_inset

 indica baja dependencia con respecto a aquellas aristas entre 
\begin_inset Formula $Pa_{v_{j}}$
\end_inset

 y 
\begin_inset Formula $v_{j}$
\end_inset

.
 Se estableció un parámetro para definir el valor mínimo de dependencia
 entre un nodo hijo y sus padres (
\emph on
near-independence
\emph default
) el cual llamamos 
\begin_inset Formula $\varepsilon-independence$
\end_inset

, El escenario de aplicación de esta estrategía durante la simulación MCMC
 fue así:
\end_layout

\begin_layout Enumerate

\change_inserted 2089656932 1565010122
Antes de adicionar cada arista se valida la dependencia entre el nodo destino
 
\begin_inset Formula $v_{j}$
\end_inset

 de la arista y el origen 
\begin_inset Formula $v_{i}$
\end_inset

 con respecto a los padres previos de dicho nodo 
\begin_inset Formula $Pa_{v_{j}}$
\end_inset

, por medio de la siguiente comparación:
\begin_inset Formula 
\[
P(\nu_{j},Pa_{\nu_{j}}|\nu_{i})-P(\nu_{j}|Pa_{\nu_{j}})\geq\varepsilon-independence
\]

\end_inset

, donde 
\begin_inset Formula $v_{i},v_{j}$
\end_inset

 son el nodo origen y destino respectivamente de la arista que se intenta
 adicionar.
\end_layout

\begin_layout Enumerate

\change_inserted 2089656932 1565010122
En los casos que se cumplio la condición anterior, la simulación continuo.
\end_layout

\begin_layout Enumerate

\change_inserted 2089656932 1565010122
En los demás casos, la operación fue ignorada.
\end_layout

\begin_layout Subsection
Simulación 
\change_deleted 2089656932 1563663940
I
\change_inserted 2089656932 1563663942
i
\change_unchanged
terativa
\change_inserted 2089656932 1563690897
 
\begin_inset CommandInset label
LatexCommand label
name "sec:simulaIiterativa"

\end_inset


\change_unchanged

\end_layout

\begin_layout Standard
La estrategía
\change_inserted 2089656932 1565013909
 para cosolidar los modelos
\change_unchanged
 ut
\change_inserted 2089656932 1564920129
i
\change_unchanged
lizada en esta técnica de aprendizaje fue promediar todas las redes 
\change_inserted 2089656932 1563663950
ó
\change_deleted 2089656932 1563663950
o
\change_unchanged
ptimas encontradas en el proceso que se caracterizó en la
\change_inserted 2089656932 1565007872
s
\change_unchanged
 secci
\change_inserted 2089656932 1565007876
o
\change_deleted 2089656932 1565007875
ó
\change_unchanged
n
\change_inserted 2089656932 1565007877
es
\change_unchanged
 anterior
\change_inserted 2089656932 1565007882
es
\change_unchanged
 (MCMC).
 Sin embargo, por la naturaleza de las caminatas aleatorias que realiza
 la simulación MCMC, es posible que existan estructuras 
\change_inserted 2089656932 1563663965
ó
\change_deleted 2089656932 1563663965
o
\change_unchanged
ptimas sin explorar en el espacio de búsqueda.
 Para dar solución a la situación anterior se elaboró un algorimtmo iterativo
 que reinicia el proceso de simulación MCMC a partir de una estructura de
 la red desconectada, para lograr que este proceso pueda tomar rutas alternativa
s en su caminata aleatoria
\change_inserted 2089656932 1564920463
,
\change_unchanged
 y así encontrar nuevas estructuras de red 
\change_inserted 2089656932 1565013963
ó
\change_deleted 2089656932 1565013962
o
\change_unchanged
ptimas.
 La condición de parada del algoritmo iterativo de simulación MCMC es no
 encontrar un porcentaje mayor que 
\begin_inset Formula $P$
\end_inset

 
\change_deleted 2089656932 1563663998
de
\change_unchanged
 redes nuevas (en las experimentaciones se fijo 
\begin_inset Formula $P=10\%$
\end_inset

)
\change_inserted 2089656932 1565014159
.
 Los ejemplo que se presentan más adelante en esta sección emplean la simulación
 iterativa como parte de la construcción de una red bayesiana minimal y
 reducida.

\change_deleted 2089656932 1565014014
 
\change_inserted 2089656932 1563664392

\end_layout

\begin_layout Subsection
Redes Ponderadas (Weighted networks) 
\end_layout

\begin_layout Standard

\change_deleted 2089656932 1563690709
Explicar como se ponderan las redes (art.
 Ecoli)
\change_unchanged

\end_layout

\begin_layout Standard
Como se meciona anteriormente, parte estratégica del aprendizaje fue promediar
 todas las redes optimas encontradas por el algoritmo iterativo de simulación
 MCMC (
\change_inserted 2089656932 1563690878
Ver la sección 
\change_unchanged
Ref.
 sección
\change_inserted 2089656932 1563690917
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:simulaIiterativa"
plural "false"
caps "false"
noprefix "false"

\end_inset


\change_unchanged
 
\change_deleted 2089656932 1563690921

\begin_inset Note Note
status open

\begin_layout Plain Layout
1.3
\end_layout

\end_inset


\change_unchanged
).
 
\change_deleted 2089656932 1563690934
Sin embargo, no todas las redes encontradas fueron promedidas
\change_inserted 2089656932 1563690951
 Para decidir si una red es óptima, se utilizó su probabilidad posterior
\change_unchanged
.
 El criterio para seleccionar los conjuntos de redes a promediar de cada
 simulación MCMC, fue definir un rango 
\begin_inset Formula $[minLogPosterior,maxLogPoserior]$
\end_inset

, dicho rango se determin
\change_inserted 2089656932 1563690989
ó
\change_deleted 2089656932 1563690987
o
\change_unchanged
 de la siguiente manera:
\end_layout

\begin_layout Itemize
Antes de determinar estos valores se hicieron los siguientes calculos:
\end_layout

\begin_deeper
\begin_layout Itemize
Se análizaron los valores del logaritmo de la distribución posterior de
 cada red encontrada durante las simulaciones MCMC.
\end_layout

\begin_layout Itemize
Se construy
\change_inserted 2089656932 1563691012
ó
\change_deleted 2089656932 1563691004
o
\change_unchanged
 el conjunto de los valores máximos 
\change_deleted 2089656932 1563691022
analizados en el punto anterior
\change_unchanged
 para cada simulación del algoritmo iterativo de simulación MCMC: 
\begin_inset Formula $\{maxLogPos_{1},..,maxLogPos_{M}\}$
\end_inset

,
\change_inserted 2089656932 1563691031
 
\change_unchanged
siendo 
\begin_inset Formula $M$
\end_inset

 la última iteración del algoritmo en mención.
 
\end_layout

\end_deeper
\begin_layout Itemize

\change_inserted 2089656932 1563691167
Para determinar el valor de 
\begin_inset Formula $maxLogPosterior$
\end_inset

 se seleccionó el valor 
\emph on
mayor
\emph default
 del conjunto de valores máximos análizados anteriormente: 
\begin_inset Formula $maxLogPosterior=max\{maxLogPos_{1},..,maxLogPos_{M}\}$
\end_inset

 
\end_layout

\begin_layout Itemize
Para determinar el valor de 
\begin_inset Formula $minLogPosterior$
\end_inset

 se seleccionó el valor 
\emph on
menor
\emph default
 del conjunto de valores máximos análizados en el punto anterior
\change_inserted 2089656932 1563691216
: 
\begin_inset Formula $minLogPosterior=min\{maxLogPos_{1},..,maxLogPos_{M}\}$
\end_inset


\change_deleted 2089656932 1563691156

\end_layout

\begin_layout Itemize

\change_deleted 2089656932 1563691156
Para determinar el valor de 
\begin_inset Formula $maxLogPosterior$
\end_inset

 se seleccionó el valor 
\emph on
mayor
\emph default
 del conjunto de valores máximos análizados anteriormente 
\change_unchanged

\end_layout

\begin_layout Standard
Una vez que se obtine el conjunto de redes optimas, 
\change_deleted 2089656932 1563691240
es decir, todas las redes encontradas y con logaritmo de la posterior en
 el intervalo 
\change_unchanged

\begin_inset Formula $[minLogPosterior,maxLogPoserior]$
\end_inset

, se descartan las redes repetidas y se obtienen los pesos de la siguiente
 manera:
\end_layout

\begin_layout Itemize
Se cuentan las ocurrencias de cada arista a lo largo del conjunto de redes
 optimas
\end_layout

\begin_layout Itemize
Para cada arista, se divide el conteo realizado en el punto anterior sobre
 la cardinalidad del conjunto de redes 
\change_deleted 2089656932 1563691340
o
\change_inserted 2089656932 1563691342
ó
\change_unchanged
ptimas.
\end_layout

\begin_layout Standard
Finalmente, se define la
\emph on
 red ponderada
\emph default
 como la red constituida las aristas y los pesos obtenidos por el procedimiento
 descrito anteriormente.

\change_inserted 2089656932 1563691362
 Es de notar que la red ponderada ya no es necesariamente una red Bayesiana,
 ya que al considerar todas las aristas que ocurren en las redes óptimas,
 se pueden producir ciclos.
\change_unchanged

\end_layout

\begin_layout Subsection
Obtención de la red Bayesiana minimal
\end_layout

\begin_layout Standard

\change_deleted 2089656932 1563691385
Una primera observación con respecto al procedimiento descrito en la sección
 anterior para obtener redes ponderadas es que la red resultante no es una
 red Bayesiana.
 
\change_unchanged
Para
\change_deleted 2089656932 1563691394
 efectos de poder 
\change_unchanged
realizar analisis causales sobre las variables de estudio
\change_inserted 2089656932 1563691403
,
\change_unchanged
 
\change_deleted 2089656932 1563691408
y
\change_unchanged
 basados en los datos observados es importante resumir los resultados del
 aprendizaje en una red Bayesiana.
 En esta sección presentamos un método para obtener una 
\emph on
red Bayesiana minimal
\emph default
 resultado del aprendizaje basado en los datos observados
\change_inserted 2089656932 1563691485
.
 Para este efecto, reducimos la red panderada
\change_unchanged
 
\change_deleted 2089656932 1563691499
y consiste de
\change_unchanged
 
\change_inserted 2089656932 1563691509
aplicando 
\change_unchanged
los siguientes pasos:
\end_layout

\begin_layout Enumerate
Con base en la red ponderada construida con el método descrito en la sección
 anterior, procedemos a encontrar la arista con el mayor peso de toda la
 red, es decir, la arista que tuvo más ocurrencias 
\change_deleted 2089656932 1563691534
(o mayor frecuencia)
\change_unchanged
 en el conjunto de redes 
\change_deleted 2089656932 1563691540
o
\change_inserted 2089656932 1563691542
ó
\change_unchanged
ptimas
\change_inserted 2089656932 1563691557
,
\change_unchanged
 tambi
\change_deleted 2089656932 1563691549
e
\change_inserted 2089656932 1563691552
é
\change_unchanged
n obtenido con el procedimietno de la sección anterior.
 El resultado de este paso será entonces los nodos origen y destino correspondie
ntes a la arista: 
\begin_inset Formula $i,j$
\end_inset

 y su peso 
\begin_inset Formula $w_{ij}.$
\end_inset


\end_layout

\begin_layout Enumerate
Ahora, con base en el resultado del paso anterior se filtra el conjunto
 de redes 
\change_inserted 2089656932 1563691573
ó
\change_deleted 2089656932 1563691573
o
\change_unchanged
ptimas dejando unicamente las redes donde aparece la arista 
\begin_inset Formula $i,j$
\end_inset

.
 Paso aseguir, se repite el procedimiento para obtener una red ponderada
 pero esta vez con el conjunto de redes optimas 
\change_deleted 2089656932 1563691590
filtrado como se acaba de indicar
\change_inserted 2089656932 1563691636
 que usan la arista que va del nodo 
\begin_inset Formula $i$
\end_inset

 al nodo 
\begin_inset Formula $j$
\end_inset

 
\change_unchanged
.
\end_layout

\begin_layout Enumerate
Se repiten los pasos 1 y 2 hasta que todas las aristas de la red ponderada
 tengan peso 
\begin_inset Formula $1$
\end_inset

.
\end_layout

\begin_layout Standard
La red 
\change_deleted 2089656932 1563691646
Bayesiana
\change_unchanged
 resultante es una red que pertenece al conjunto de redes optimas, por lo
 tanto, es una red Bayesiana y le llamamos 
\emph on
minimal
\change_inserted 2089656932 1563691669

\emph default
, porque es la intersección de una sucesión redes Bayesianes
\change_deleted 2089656932 1563691681
.
 pues sus aristas son las que tienen mayor frecuencia u ocurrencia en dicho
 conjunto
\change_unchanged
.
\change_inserted 2089656932 1563691868

\end_layout

\begin_layout Subsection

\change_inserted 2089656932 1565006699
Ejemplo de red Bayesiana minimal
\change_unchanged

\end_layout

\begin_layout Standard
Para ilustrar este procedimiento, retomaremos de nuevo el ejemplo de aprendizaje
 con datos de expresión génica de E.coli.
 En esta ocación tomaremos uno de los cluster identificados en el cap
\change_inserted 2089656932 1565006239
í
\change_deleted 2089656932 1565006238
i
\change_unchanged
tulo anterior
\change_inserted 2089656932 1565006040
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:coexpressionNet"
plural "false"
caps "false"
noprefix "false"

\end_inset

)
\change_unchanged
 cuando se construyo la red de co-expresión génica.
 El 
\change_deleted 2089656932 1565006134
primer
\change_unchanged
 cluster 
\change_deleted 2089656932 1565006139
estuvo
\change_inserted 2089656932 1565006139
esta
\change_unchanged
 compuesto por los genes: 
\emph on
malQ, malT, lamB,malG,malP,malM,malE,malF y malK
\emph default
 (ver
\change_inserted 2089656932 1563692170
 en la tabla 
\begin_inset CommandInset ref
LatexCommand ref
reference "table:genesAnnotations"
plural "false"
caps "false"
noprefix "false"

\end_inset

, el cluster 34-darkmagenta
\change_deleted 2089656932 1563692176
 cap.
 2
\change_unchanged
), adicionalmente, fueron seleccionados
\change_inserted 2089656932 1563692208
 aleatoriamente los genes de
\change_unchanged
 dos clusters de control 
\change_inserted 2089656932 1563692220
cada uno 
\change_unchanged
con 9 genes 
\change_deleted 2089656932 1563692217
cada uno
\change_unchanged
 
\change_deleted 2089656932 1563692228
y seleccionados
\change_unchanged
 
\change_deleted 2089656932 1563692198
aleatoriamente
\change_unchanged
.
 Los resultados
\change_inserted 2089656932 1563692247
 de esta experimentación
\change_unchanged
 pueden verse en la figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Ecoli9comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\change_inserted 2089656932 1563692301
donde las 3 redes tienen 
\change_deleted 2089656932 1563692304
con
\change_unchanged
 una estructura identica y 
\change_deleted 2089656932 1563692315
resaltando
\change_inserted 2089656932 1563692338
 se evidencia
\change_unchanged
 que el aprendizaje finalmente lo que establece es el orden en que los nodos
 son colgados en la estructura.
 En las estructuras siempre hay un nodo padre que tiene aristas con todos
 los demas nodos (8 nodos hijos), y en la jerarquía le sigue un hijo que
 es padre de los demas (7 nodos hijos), y asi sucesivamente, observandose,
 que en esta jerarqui no se podrian tener más aristas (36 aristas) ya que
 entonces no sería un grafo aciclico, es decir, que la optimización no puede
 dar con una estructura de red 
\change_deleted 2089656932 1563692379
más optima
\change_inserted 2089656932 1563692384
 mejor
\change_unchanged
 (con más aristas) que esta.
 Como se vera en el siguiente cap
\change_inserted 2089656932 1565006230
í
\change_deleted 2089656932 1565006230
i
\change_unchanged
tulo esta situación se evidencia también en redes con 4 genes, 
\change_inserted 2089656932 1563692521
5
\change_deleted 2089656932 1563692521
6
\change_unchanged
 genes y 11 genes
\change_inserted 2089656932 1563692685
 (ver en la tabla 
\begin_inset CommandInset ref
LatexCommand ref
reference "table:genesAnnotations"
plural "false"
caps "false"
noprefix "false"

\end_inset

, los clusters: 81-salmon2, 73-firebrick4 y 40-mediumpurple3, respectivamente).

\change_unchanged
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparación entre las redes Bayesianas minimales con 9 genes.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Red Bayesiana minimal para los genes del cluster de transporte de maltose.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/Ecoli9maltose.png
	scale 30

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Ecoli9maltose"

\end_inset


\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Red Bayesiana minimal para los genes del cluster de genes aleatorios del
 control 1.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Ecoli9ctrl1"

\end_inset


\begin_inset Graphics
	filename images/Ecoli9ctrl1.png
	scale 30

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Red Bayesiana minimal para los genes del cluster de genes aleatorios del
 control 2.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Ecoli9ctrl2"

\end_inset


\begin_inset Graphics
	filename images/Ecoli9ctrl2.png
	scale 30

\end_inset


\end_layout

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Ecoli9comparison"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Obtención de la red Bayesiana reducida
\end_layout

\begin_layout Standard
Para facilitar los análisis 
\change_deleted 2089656932 1565006435
para
\change_inserted 2089656932 1565006435
y
\change_unchanged
 encontrar carácteristicas interesantes biológicamente en las redes Bayesianas
 minimales, aplicamos un procedimiento adicional para dar con una red Bayesiana
 minimal 
\emph on
reducida, 
\emph default
aplicando el siguiente procedimiento
\emph on
:
\end_layout

\begin_layout Enumerate
Con base en los pesos originales de las aristas (en la primera red ponderada
 obtenida antes de calcular la red Bayesiana minimal) se descarta la arista
 que ten
\change_inserted 2089656932 1563692719
í
\change_deleted 2089656932 1563692718
i
\change_unchanged
a el menor peso siempre y cuando esta operación no deje que la red se desconecte
 de alg
\change_inserted 2089656932 1565006507
ú
\change_deleted 2089656932 1565006502
u
\change_unchanged
n nodo.
\end_layout

\begin_layout Enumerate
Se repite el paso 1 hasta donde sea posible siempre cuidando la condición
 de que la red resultante no se desconecte de algun modo.
\change_inserted 2089656932 1563692952

\end_layout

\begin_layout Subsection

\change_inserted 2089656932 1565006614
Ejemplo de red Bayesiana reducida
\change_unchanged

\end_layout

\begin_layout Standard
Pasando a la practica nuevamente con el ejemplo de datos de expresión génica
 de E.coli, pudimos ver en la sección anterior una red menos sobre-entrenada
 y dificil aun de análizar biológicamente.
 Ahora aplicando el procedimiento de reducción pod
\change_deleted 2089656932 1563692756
r
\change_unchanged
emos ver algunos ejemplos de resultados coherentes y más prestos para un
 análisis causal entre genes según su función génica y anotaciones GO, ver
 la figura 
\begin_inset CommandInset ref
LatexCommand ref
reference "EcoliMaltoseReduced"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\change_inserted 2089656932 1563692843

\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Red Bayesiana minimal reducida de los genes del cluster de transporte de
 maltose en E.coli.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename images/EcoliMaltoseReduced.png
	scale 30

\end_inset


\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "EcoliMaltoseReduced"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Comentarios concluyentes
\change_inserted 2089656932 1563737682

\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565021110
En la experimentación dusrante la estancia doctoral en UNAM se mezclaron
 genes de diferentes clusters y los resultados presentaron sobreentrenamiento,
 ya que al tratarse de genes que pertenecen a diferentes clusters se representan
 con variables aleatorias independientes por lo que al poner aristas sobre
 estos nodos los puntajes (del inglés scores) de los modelos sobreentrenados
 mejoran y prevalecen.
 Se recomiendan los análsis con redes Bayesianas sobre clusters de variables
 que presenten alguna dependencia (co-experesión para el caso de E.coli)
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565020026
Como una mejora de la simulación MCMC se propusó el método de simulación
 iterativa, después de analizar que los caminos de exploración del espacio
 de búsqueda en un punto de la simulación MCMC, pueden quedar restringidos
 por el rumbo tomado y por la consigna de no generación de bubles en las
 redes del conjunto solución, y encontramos un equilibrio (trade-off) entre
 el número de simulaciones suficientes para agregar nuevos modelos, pero
 sin impactar demasiado la complejidad del problema, estableciendo como
 criterio de parada de las iteraciones, el porcentaje mínimo de nuevos modelos
 para agregar al conjunto solución.
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565018652
El método de discretización por cuantiles nos permitío darle manejo a las
 variables continuas dentro del caso discreto tornandose un proceso operativo
 dendro del flujo de trabajo del framework.
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565016354
Los tiempos computaciones de las simulaciones MCMC se veran afectados por
 el número de variables 
\begin_inset Formula $N$
\end_inset

 ya que al evaluarse la posterior para el nodo hoja de la red Bayesiana
 minimal (que es una de las redes óptimas) la complejidad será exponencial
 : 
\begin_inset Formula $O(3^{N-1})$
\end_inset

 , donde 
\begin_inset Formula $3$
\end_inset

 es el número de cuantiles en la discretización y 
\begin_inset Formula $N-1$
\end_inset

 el número de padres del nodo hoja.
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565015774
La red ponderada aunque no es una red Bayesiana se puede utilizar para análizar
 la frecuencia de ocurrencia de las aristas del conjunto de redes óptima
 y aunque se tarta de un análisis meramente cuantitativo resultó muy util
 en la costrucción dela red Bayesiana minimal y reducida.
 Esta red ponderada fue una sugerencia para análisis comparativos durante
 la estancia doctoral en UNAM.
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565015271
Inherente al aprendizaje de redes Bayesianas es la tendencia a sobre-entrenar
 (overfitting) las redes, en todos los modelos se busca optimizar la función
 score (likelihood), la cual mejora cuando se agregan nuevas aristas, pero
 tiende al sobre-entrenamiento (overfitting), es decir, entre más aristas
 se agregan, la red mejora su puntaje (o score), sin embargo, la simplicidad
 de un modelo es deseable y con el score mas alto posible.
 El aprendizaje de redes de Bayes tiene esta tendencia ya que su técnica
 se enfoca por optimización.
\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565015375
El método para obtener la red Bayesiana minimal entrega una red Bayesiana
 que consolida el conjunto de redes óptimas resultado de la simulación iterativa
 y sin perder el poder análitico del modelo de red Bayesiana.
\change_unchanged

\end_layout

\begin_layout Itemize

\change_deleted 2089656932 1565014900
Por ultimo, se concluye que a
\change_inserted 2089656932 1565014900
A
\change_unchanged
l aplicar el procedimie
\change_inserted 2089656932 1565015238
n
\change_unchanged
t
\change_deleted 2089656932 1565015239
n
\change_unchanged
o para obtener una red Bayesiana minimal 
\change_deleted 2089656932 1565014941
reducida
\change_unchanged
 y apoyarse en las anotaciones GO, 
\change_deleted 2089656932 1563693623
se obtuvieron 
\change_inserted 2089656932 1563693631
es posible obtener 
\change_unchanged
resultados coherentes al menos con los clusters conocidos con los que 
\change_deleted 2089656932 1563693730
se
\change_inserted 2089656932 1563693732
fueron
\change_unchanged
 
\change_deleted 2089656932 1563693751
realizados
\change_inserted 2089656932 1563693753
desarrollados
\change_unchanged
 los 
\change_deleted 2089656932 1563693757
experimentos
\change_inserted 2089656932 1565014819
ejemplos de 
\change_deleted 2089656932 1563693777
: hydrogenase,
\change_unchanged
 maltose
\change_inserted 2089656932 1565014816
.
\change_deleted 2089656932 1563693785
, lactose, arabinose y maquinaria basal, entre otros
\change_unchanged
.
\change_inserted 2089656932 1565014946

\end_layout

\begin_layout Itemize

\change_inserted 2089656932 1565015181
La red minimal aunque optima sigue pareciendo sobreentrenada y la red Bayesiana
 reducida logra mitigar el sobreentrenamiente eliminando las aristas con
 menos frecuencia de ocurrencia en los conjuntos de redes óptimas encontradas
 durante la simulación iterativa, pero sin dejar desconexa la red Bayesina.
 Por esta razón es una buen proceso para obtener un modelo más presto para
 análisis bioinformático.
\change_unchanged

\end_layout

\end_body
\end_document
